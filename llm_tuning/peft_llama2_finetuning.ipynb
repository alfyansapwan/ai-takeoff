{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - LLaMA2 (PEFT Finetuning)\n",
    "\n",
    "Adapted from: [Llama 2 PEFT Finetuning with Text Data on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_peft_finetuning.ipynb)\n",
    "\n",
    "Modified by: [Wan Qi Ang](https://github.com/angwanqi) for 2024 EDB x Google Cloud - Cloud AI Take Off Program\n",
    "\n",
    "Last updated: 11 Nov 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates downloading [LLaMA2 models](https://huggingface.co/meta-llama), finetuning with parameter efficient finetuning libraries ([PEFT](https://github.com/huggingface/peft)), and deploying the finetuned model on Vertex AI.\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Download prebuilt LLaMA2 models.\n",
    "- Finetune and deploy LLaMA2 models with Vertex AI SDK.\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Run the cell below if this is your first time running the notebook. Else, feel free to skip the cell below as the libraries would have already been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform\n",
    "%pip install --quiet --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart current runtime\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<b>NOTE:</b> Only restart the current runtime if you installed libraries. If you did not install new libraries, you do not need to restart the kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the default cloud project id.\n",
    "PROJECT_ID= !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Persistent Resource ID\n",
    "PERSISTENT_RESOURCE_ID = \"ai-takeoff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent Resource found in us-central1\n"
     ]
    }
   ],
   "source": [
    "# Dynamically retrieve Persistent Resource location\n",
    "PERSISTENT_RESOURCE_REGION = \"\"\n",
    "check_regions = [\"us-central1\", \"asia-southeast1\", \"europe-west4\"]\n",
    "\n",
    "for region in check_regions:\n",
    "    shell_output = !gcloud ai persistent-resources list --project=$PROJECT_ID --region=$region\n",
    "    if \"Listed 0 items.\" not in shell_output:\n",
    "        print(f\"Persistent Resource found in {region}\")\n",
    "        PERSISTENT_RESOURCE_REGION = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account is NOT ENABLED on Persistent Resource\n"
     ]
    }
   ],
   "source": [
    "# Check if Service Account is enabled on Persistent Resource\n",
    "SA_ENABLED = \"\"\n",
    "\n",
    "DESCRIBE_PR_OUTPUT = !gcloud ai persistent-resources describe $PERSISTENT_RESOURCE_ID --project=$PROJECT_ID --region=$PERSISTENT_RESOURCE_REGION\n",
    "PR_DETAILS = \" \".join(DESCRIBE_PR_OUTPUT)\n",
    "\n",
    "if \"enableCustomServiceAccount: true\" in PR_DETAILS:\n",
    "    SA_ENABLED = True\n",
    "    print(f\"Service Account is ENABLED on Persistent Resource\")\n",
    "else:\n",
    "    SA_ENABLED = False\n",
    "    print(f\"Service Account is NOT ENABLED on Persistent Resource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: bb-genai\n",
      "Project Region: us-central1\n"
     ]
    }
   ],
   "source": [
    "# Set the default region for launching jobs.\n",
    "REGION = PERSISTENT_RESOURCE_REGION\n",
    "\n",
    "print(f\"Project ID:\", PROJECT_ID)\n",
    "print(f\"Project Region:\", REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set (or create) the Google Cloud Storage bucket\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>INPUT REQUIRED:</b> Replace <YOUR_NAME> with your name so that you'll be able to identify your Google Cloud Storage bucket later on. <b>Example:</b> BUCKET_PREFIX = \"john\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_BUCKET_URI: gs://wq-llama2-text-peft\n",
      "STAGING_BUCKET_URI: gs://wq-llama2-text-peft/temporal\n",
      "EXPERIMENT_BUCKET_URI: gs://wq-llama2-text-peft/peft\n",
      "MODEL_BUCKET_URI: gs://wq-llama2-text-peft/llama2\n"
     ]
    }
   ],
   "source": [
    "# Update the bucket prefix with your name\n",
    "PREFIX = \"<YOUR_NAME>\"\n",
    "\n",
    "# Concatenate to get the full bucket name\n",
    "BUCKET_NAME = PREFIX + \"-llama2-text-peft\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"llama2\")\n",
    "\n",
    "\n",
    "print(f\"ROOT_BUCKET_URI:\", BUCKET_URI)\n",
    "print(f\"STAGING_BUCKET_URI:\", STAGING_BUCKET)\n",
    "print(f\"EXPERIMENT_BUCKET_URI:\", EXPERIMENT_BUCKET)\n",
    "print(f\"MODEL_BUCKET_URI:\", MODEL_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://wq-llama2-text-peft/...\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.storage.buckets.create) HTTPError 409: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "# Create Cloud Storage Bucket\n",
    "!gcloud storage buckets create $BUCKET_URI --location=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up the Compute Engine Service Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the default Compute Engine Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this default Service Account: 951134149941-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# Gets the default BUCKET_URI and SERVICE_ACCOUNT if they were not specified by the user.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign Cloud Storage admin IAM role to the Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://wq-llama2-text-peft/\n"
     ]
    }
   ],
   "source": [
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Vertex AI API.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Llama 2\n",
    "- The original models from Meta are converted into the Hugging Face format for serving in Vertex AI.\n",
    "- Accept the model agreement to access the models:\n",
    "- Open the [LLaMA2 model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/139) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
    "- Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
    "- A Cloud Storage bucket (starting with `gs://`) containing LLaMA 2 pretrained and finetuned models will be shared under the “Documentation” section and its “Get started” subsection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download Llama 2 Model Artifacts to your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Llama 2 model artifaces to your bucket\n",
    "VERTEX_AI_MODEL_GARDEN_LLAMA2 = \"gs://vertex-model-garden-public-us-central1/llama2\"\n",
    "\n",
    "! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA2/* $MODEL_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "id": "zI30m3bqDtCj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up the training dataset\n",
    "This notebook uses [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset as an example.\n",
    "You can set `dataset_name` to any existing [Hugging Face dataset](https://huggingface.co/datasets) name, and set `instruct_column_in_dataset` to the name of the dataset column containing training data. The [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) has only one column `text`, and therefore we set `instruct_column_in_dataset` to `text` in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "Ax6GlzzMk-sc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hugging Face dataset name or gs:// URI to a custom JSONL dataset.\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "\n",
    "# Name of the dataset column containing training text input.\n",
    "instruct_column_in_dataset = \"text\"\n",
    "\n",
    "# Optional. Template name or gs:// URI to a custom template.\n",
    "template = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### Human: Can you write a short introduction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### Human: ¿CUales son las etapas del desarrol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  ### Human: Can you write a short introduction ...\n",
       "1  ### Human: ¿CUales son las etapas del desarrol..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and have a look at what's in it\n",
    "splits = {'train': 'openassistant_best_replies_train.jsonl', 'test': 'openassistant_best_replies_eval.jsonl'}\n",
    "df = pd.read_json(\"hf://datasets/timdettmers/openassistant-guanaco/\" + splits[\"train\"], lines=True)\n",
    "\n",
    "# Print the first 2 rows\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the first row of data\n",
    "df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finetune with PEFT\n",
    "This section demonstrates how to finetune the LLaMA 2 models with PEFT LoRA. By default, the model will be finetuned for 500 steps on a batch size of 1 to save GPU resources.\n",
    "- Finetuning `llama2-7b` models is expected to take around 30 minutes.\n",
    "- To customize finetuning settings and parameters, click \"Show code\" to see more details.\n",
    "\n",
    "Here are the full list of models that you can tune:\\\n",
    "[\"llama2-7b-hf\", \"llama2-7b-chat-hf\", \"llama2-13b-hf\", \"llama2-13b-chat-hf\", \"llama2-70b-hf\", \"llama2-70b-chat-hf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set the base model ID that you would like to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the base model id.\n",
    "base_model_id = \"llama2-7b-hf\" \n",
    "model_id = os.path.join(MODEL_BUCKET, base_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set the accelerator that you would like to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the accelerator type.\n",
    "accelerator_type = \"NVIDIA_A100_80GB\"\n",
    "\n",
    "machine_type = None\n",
    "if \"7b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
    "        machine_type = \"n1-standard-16\"\n",
    "        accelerator_count = 2\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-48\"\n",
    "        accelerator_count = 4\n",
    "    elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
    "        machine_type = \"a2-ultragpu-1g\"\n",
    "        accelerator_count = 1 \n",
    "elif \"13b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
    "        machine_type = \"n1-standard-32\"\n",
    "        accelerator_count = 4\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-24\"\n",
    "        accelerator_count = 2\n",
    "    elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
    "        machine_type = \"a2-ultragpu-1g\"\n",
    "        accelerator_count = 1\n",
    "elif \"70b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-4g\"\n",
    "        accelerator_count = 4\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-96\"\n",
    "        accelerator_count = 8\n",
    "\n",
    "if machine_type is None:\n",
    "    raise ValueError(\n",
    "        f\"Recommended machine settings not found for: {accelerator_type}. To use another another accelerator, please edit this code block to set an appropriate `machine_type`, `accelerator_type`, and `accelerator_count` in worker_pool_specs.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set up the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "id": "e1289e21a9d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_name = get_job_name_with_datetime(PREFIX + \"-llama2-train\")\n",
    "output_dir = os.path.join(EXPERIMENT_BUCKET, job_name)\n",
    "merge_job_name = get_job_name_with_datetime(PREFIX + \"-llama2-merge\")\n",
    "merged_model_output_dir = os.path.join(EXPERIMENT_BUCKET, merge_job_name)\n",
    "finetune_precision_mode = \"float16\"\n",
    "\n",
    "# The pre-built training docker images.\n",
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:latest\"\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "# Runs 500 training steps.\n",
    "max_steps = 500\n",
    "per_device_train_batch_size = 1\n",
    "# LoRA parameters.\n",
    "lora_rank = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "\n",
    "flags = {\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"precision_mode\": finetune_precision_mode,\n",
    "    \"task\": \"instruct-lora\",\n",
    "    \"per_device_train_batch_size\": per_device_train_batch_size,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"instruct_column_in_dataset\": instruct_column_in_dataset,\n",
    "    \"template\": template,\n",
    "    \"pretrained_model_id\": model_id,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"merge_base_and_lora_output_dir\": merged_model_output_dir,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"lora_rank\": lora_rank,\n",
    "    \"lora_alpha\": lora_alpha,\n",
    "    \"lora_dropout\": lora_dropout,\n",
    "}\n",
    "\n",
    "train_job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=[\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "                \"accelerator_type\": accelerator_type,\n",
    "                \"accelerator_count\": accelerator_count,\n",
    "            },\n",
    "            \"replica_count\": replica_count,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": TRAIN_DOCKER_URI,\n",
    "                \"args\": [\"--{}={}\".format(k, v) for k, v in flags.items()],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    "    persistent_resource_id=PERSISTENT_RESOURCE_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Service Account Enablement on Vertex Persistent Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SA_ENABLED:\n",
    "    print(\"Please proceed to next step to start the training job!\")\n",
    "else:\n",
    "    print(\"Please open the Terminal in a new tab and run the command -> gcloud auth login```\")\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Once done, continue on to start the training job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ It will take ~45mins for the model tuning job to complete on the provided dataset and set configurations/hyperparameters. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training job with user account\n",
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/951134149941/locations/us-central1/customJobs/5975292470815096832\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/951134149941/locations/us-central1/customJobs/5975292470815096832')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5975292470815096832?project=951134149941\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/951134149941/locations/us-central1/customJobs/5975292470815096832 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning training job with user account\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrain_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe finetuned models of different trials can be found at: \u001b[39m\u001b[38;5;124m\"\u001b[39m, output_dir)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe finetuned model merged with the base model can be found at: \u001b[39m\u001b[38;5;124m\"\u001b[39m,merged_model_output_dir,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/jobs.py:2317\u001b[0m, in \u001b[0;36mCustomJob.run\u001b[0;34m(self, service_account, network, timeout, restart_job_on_worker_restart, enable_web_access, experiment, experiment_run, tensorboard, sync, create_request_timeout, disable_retries, persistent_resource_id, scheduling_strategy, max_wait_duration)\u001b[0m\n\u001b[1;32m   2314\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[1;32m   2315\u001b[0m service_account \u001b[38;5;241m=\u001b[39m service_account \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mservice_account\n\u001b[0;32m-> 2317\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrestart_job_on_worker_restart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestart_job_on_worker_restart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_web_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_web_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersistent_resource_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersistent_resource_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduling_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduling_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_wait_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_duration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/jobs.py:2440\u001b[0m, in \u001b[0;36mCustomJob._run\u001b[0;34m(self, service_account, network, timeout, restart_job_on_worker_restart, enable_web_access, experiment, experiment_run, tensorboard, sync, create_request_timeout, disable_retries, persistent_resource_id, scheduling_strategy, max_wait_duration)\u001b[0m\n\u001b[1;32m   2352\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run the configured CustomJob.\u001b[39;00m\n\u001b[1;32m   2353\u001b[0m \n\u001b[1;32m   2354\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;124;03m        the job will wait indefinitely. The default is 1 day.\u001b[39;00m\n\u001b[1;32m   2423\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   2425\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[1;32m   2426\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2437\u001b[0m     max_wait_duration\u001b[38;5;241m=\u001b[39mmax_wait_duration,\n\u001b[1;32m   2438\u001b[0m )\n\u001b[0;32m-> 2440\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/jobs.py:1630\u001b[0m, in \u001b[0;36m_RunnableJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1628\u001b[0m         previous_time \u001b[38;5;241m=\u001b[39m current_time\n\u001b[1;32m   1629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_web_access_uris()\n\u001b[0;32m-> 1630\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_JOB_WAIT_TIME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_job_state()\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, CustomJob):\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;66;03m# End the experiment run associated with the custom job, if exists.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if SA_ENABLED:\n",
    "    print(\"Running training job with service account\")\n",
    "    train_job.run(service_account=SERVICE_ACCOUNT)\n",
    "else:\n",
    "    print(\"Running training job with user account\")\n",
    "    train_job.run()\n",
    "\n",
    "print(\"The finetuned models of different trials can be found at: \", output_dir)\n",
    "print(\"The finetuned model merged with the base model can be found at: \",merged_model_output_dir,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying your finetuned model\n",
    "This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 4096,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.8\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        \"--max-num-batched-tokens=4096\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\"MODEL_ID\": model_id, \"DEPLOY_SOURCE\": \"notebook\"}\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        artifact_uri=model_id,\n",
    "    )\n",
    "    print(\n",
    "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    print(\"endpoint_name:\", endpoint.name)\n",
    "\n",
    "    print(\"To load this existing endpoint from a different session:\")\n",
    "    print(\"from google.cloud import aiplatform\")\n",
    "    print(\n",
    "        f'endpoint = aiplatform.Endpoint(\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}\")'\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI API.\n",
    "# us-central1, asia-southeast1, europe-west4\n",
    "REGION = \"<SET_REGION>\"\n",
    "\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "h0hGj09CuRFQ"
   },
   "outputs": [],
   "source": [
    "# The pre-built serving and training docker images.\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240326_0916_RC00\"\n",
    "\n",
    "print(\"Deploying models in: \", merged_model_output_dir)\n",
    "\n",
    "# The max_model_len must not exceed the model's context length.\n",
    "# A larger max_model_len will require more GPU memory.\n",
    "max_model_len = 2048\n",
    "machine_type = None\n",
    "accelerator_type = \"NVIDIA_A100_80GB\"\n",
    "# accelerator_type = \"NVIDIA_L4\"\n",
    "\n",
    "if \"7b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
    "        machine_type = \"n1-standard-8\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-48\"\n",
    "        accelerator_count = 4\n",
    "    elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
    "        machine_type = \"a2-ultragpu-1g\"\n",
    "        accelerator_count = 1\n",
    "elif \"13b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "        accelerator_count = 1\n",
    "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
    "        machine_type = \"n1-standard-16\"\n",
    "        accelerator_count = 2\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-24\"\n",
    "        accelerator_count = 2\n",
    "elif \"70b\" in model_id:\n",
    "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        machine_type = \"a2-highgpu-4g\"\n",
    "        accelerator_count = 4\n",
    "    elif accelerator_type == \"NVIDIA_L4\":\n",
    "        machine_type = \"g2-standard-96\"\n",
    "        accelerator_count = 8\n",
    "\n",
    "if machine_type is None:\n",
    "    raise ValueError(\n",
    "        f\"Recommended machine settings not found for: {accelerator_type}. To use another another accelerator, please edit this code block to pass in an appropriate `machine_type`, `accelerator_type`, and `accelerator_count` to the deploy_model_vllm function.\"\n",
    "    )\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"llama-vllm-serve\"),\n",
    "    model_id=merged_model_output_dir,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing the endpoint with a prediction\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
    "\n",
    "Here we use an example from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) to show the finetuning outcome:\n",
    "\n",
    "<b>Human:</b> How would the Future of AI in 10 Years look?\\\n",
    "<b>Assistant:</b> Predicting the future is always a challenging task, but here are some possible ways that AI could evolve over the next 10 years:\n",
    "- <b>Continued advancements in deep learning:</b> Deep learning has been one of the main drivers of recent AI breakthroughs, and we can expect continued advancements in this area. This may include improvements to existing algorithms, as well as the development of new architectures that are better suited to specific types of data and tasks.\n",
    "- <b>Increased use of AI in healthcare:</b> AI has the potential to revolutionize healthcare, by improving the accuracy of diagnoses, developing new treatments, and personalizing patient care. We can expect to see continued investment in this area, with more healthcare providers and researchers using AI to improve patient outcomes.\n",
    "- <b>Greater automation in the workplace:</b> Automation is already transforming many industries, and AI is likely to play an increasingly important role in this process. We can expect to see more jobs being automated, as well as the development of new types of jobs that require a combination of human and machine skills.\n",
    "- <b>More natural and intuitive interactions with technology:</b> As AI becomes more advanced, we can expect to see more natural and intuitive ways of interacting with technology. This may include voice and gesture recognition, as well as more sophisticated chatbots and virtual assistants.\n",
    "- <b>Increased focus on ethical considerations:</b> As AI becomes more powerful, there will be a growing need to consider its ethical implications. This may include issues such as bias in AI algorithms, the impact of automation on employment, and the use of AI in surveillance and policing. \n",
    "\n",
    "Overall, the future of AI in 10 years is likely to be shaped by a combination of technological advancements, societal changes, and ethical considerations. While there are many exciting possibilities for AI in the future, it will be important to carefully consider its potential impact on society and to work towards ensuring that its benefits are shared fairly and equitably.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may uncomment the code below to load an existing endpoint.\n",
    "\n",
    "# # Retrieve the Vertex AI Prediction Endpoint IDs and set it\n",
    "# endpoint = \"\"\n",
    "# check_regions = [\"us-central1\", \"asia-southeast1\", \"europe-west4\"]\n",
    "\n",
    "# for region in check_regions:\n",
    "#     all_endpoints = aiplatform.Endpoint.list(location=region)\n",
    "#     for endpoint in all_endpoints:\n",
    "#         full_endpoint = f\"projects/{PROJECT_ID}/locations/{region}/endpoints/{endpoint.name}\"\n",
    "        \n",
    "#         if \"llama-vllm-serve\" in endpoint.display_name:\n",
    "#             endpoint = aiplatform.Endpoint(full_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vgU_qYHNuy3w"
   },
   "outputs": [],
   "source": [
    "prompt = \"How would the Future of AI in 10 Years look?\"\n",
    "max_tokens = 128\n",
    "temperature = 1.0\n",
    "top_p = 0.9\n",
    "top_k = 1\n",
    "\n",
    "# Overides max_tokens and top_k parameters during inferences.\n",
    "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
    "# you can reduce the max length, such as set max_tokens as 20.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": f\"### Human: {prompt}### Assistant: \",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "    },\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clean up resources\n",
    "Delete the experiment models and endpoints to recycle the resources and avoid unnecessary continuous charges that may incur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "if train_job._gca_resource.name:\n",
    "    # Training job is submitted.\n",
    "    train_job.delete()\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete model.\n",
    "model.delete()\n",
    "\n",
    "# Delete Cloud Storage objects that were created.\n",
    "delete_bucket = False  # @param {type:\"boolean\"}\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_llama2_peft_finetuning.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
