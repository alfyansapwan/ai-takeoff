{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - Open Models (Llama 3.2 and Stable Diffusion 2.1)\n",
    "\n",
    "Author: [Wan Qi Ang](https://github.com/angwanqi) for 2024 EDB x Google Cloud - Cloud AI Take Off Program\n",
    "\n",
    "Last updated: 30 October 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how you can call open models that are deployed on Vertex AI Prediction. The 2 models in this notebook are:\n",
    " * [Llama 3.2](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3-2)\n",
    " * [Stable Diffusion 2.1](https://console.cloud.google.com/vertex-ai/publishers/stability-ai/model-garden/stable-diffusion-2-1)\n",
    "\n",
    "### Objective\n",
    "\n",
    "* Send requests to the Vertex AI Prediction Endpoint that is hosting Llama 3.2\n",
    "* Send requests to the Vertex AI Prediction Endpoint that is hosting Stable Diffusion 2.1\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "- The models are already deployed via Model Garden's one-click deploy feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Run the cell below if this is your first time running the notebook. Else, feel free to skip the cell below as the libraries would have already been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform\n",
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart current runtime\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<b>NOTE:</b> Only restart the current runtime if you installed libraries. If you did not install new libraries, you do not need to restart the kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your project ID and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default cloud project id.\n",
    "PROJECT_ID= !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "# Set the default region for launching jobs.\n",
    "REGION = 'us-central1'\n",
    "\n",
    "print(f\"Project ID:\", PROJECT_ID)\n",
    "print(f\"Project Region:\", REGION)\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "endpoints = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Vertex AI Prediction Endpoints\n",
    "\n",
    "After you've deployed your target model to a Vertex AI Prediction Endpoint, you can send requests to the endpoint with text prompts based on your `template`. Note that the first few prompts will take longer to execute. \n",
    "\n",
    "First, let's retrieved the details of the endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the Vertex AI Prediction Endpoint ID and set it\n",
    "all_endpoints = aiplatform.Endpoint.list()\n",
    "for endpoint in all_endpoints:\n",
    "    full_endpoint = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}\"\n",
    "    \n",
    "    if endpoint.display_name == \"llama-3-2-11b-vision-mg-one-click-deploy\":\n",
    "        endpoints['llama3-2'] = aiplatform.Endpoint(full_endpoint)\n",
    "        \n",
    "    if endpoint.display_name == \"stabilityai_stable-diffusion-2-1-mg-one-click-deploy\":\n",
    "        endpoints['sd2-1'] = aiplatform.Endpoint(full_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with Llama 3.2\n",
    "**Example:**\n",
    "```\n",
    "> What is a car?\n",
    "```\n",
    "\n",
    "**Note:**\n",
    "If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "nkUaMxIus6Pv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is a car?\n",
      "Output:\n",
      " A car is a road vehicle, typically with four wheels, powered by an internal combustion engine or an electric motor. Cars are used for transportation, recreation, and other purposes. They come in various shapes, sizes, and models, ranging from small hatch\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a car?\"\n",
    "\n",
    "max_tokens = 50\n",
    "temperature = 1.0\n",
    "top_p = 1.0\n",
    "top_k = 1\n",
    "\n",
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "    },\n",
    "]\n",
    "\n",
    "llama3_response = endpoints['llama3-2'].predict(instances=instances)\n",
    "for prediction in llama3_response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Images with Stable Diffusion 2.1\n",
    "\n",
    "After your model is deployed, you'll be able to generate images by sending text prompts to the endpoint. Try your hand at generating some images below! \n",
    "\n",
    "**Example:**\n",
    "```\n",
    "> A photo of an astronaut riding a horse on mars\n",
    "> A stone castle in a forest by the river\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your prompts by adding them to a prompt list\n",
    "\n",
    "comma_separated_prompt_list = \"A photo of an astronaut riding a horse on mars, A stone castle in a forest by the river\"  # @param {type: \"string\"}\n",
    "prompt_list = [x.strip() for x in comma_separated_prompt_list.split(\",\")]\n",
    "\n",
    "# [Optional] Set a negative prompt to define what you don't want to see.\n",
    "negative_prompt = \"\"\n",
    "\n",
    "# Set parameters\n",
    "height = 768\n",
    "width = 768\n",
    "num_inference_steps = 25\n",
    "guidance_scale = 7.5\n",
    "\n",
    "\n",
    "# Construct instance list\n",
    "instances = [{\"text\": prompt} for prompt in prompt_list]\n",
    "parameters = {\n",
    "    \"negative_prompt\": negative_prompt,\n",
    "    \"height\": height,\n",
    "    \"width\": width,\n",
    "    \"num_inference_steps\": num_inference_steps,\n",
    "    \"guidance_scale\": 7.5,\n",
    "}\n",
    "\n",
    "# Send prompts and parameters to the endpoint\n",
    "response = endpoint.predict(\n",
    "    instances=instances, parameters=parameters\n",
    ")\n",
    "\n",
    "# Display the generated images\n",
    "images = [\n",
    "    common_util.base64_to_image(prediction.get(\"output\"))\n",
    "    for prediction in response.predictions\n",
    "]\n",
    "display(common_util.image_grid(images, rows=math.ceil(len(images) ** 0.5)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_gemma2_deployment_on_vertex.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
